---
title: "stock market prediction using amazon dataset"
author: "Sai Lakshmi. R - 22MIA1042"
date: "`r Sys.Date()`"
output: html_document
---

```{r}
rm(list=ls())
```



```{r}
file="C:\\Users\\indum\\Downloads\\AMZN.csv"
df=read.csv(file)
df
```
```{r}
library(dplyr)
```

```{r}
glimpse(df)
```

```{r}
head(df,5)
```



```{r}
tail(df,5)
```

```{r}
summary(df)
```

```{r}
range_high_low = max(df$High) - min(df$Low);range_high_low
```

```{r}
# Function to fill NA with the mean of the column
fill_na_with_mean <- function(df) {
  df %>%
    mutate(
      Open = if_else(is.na(Open), mean(Open, na.rm = TRUE), Open),
      High = if_else(is.na(High), mean(High, na.rm = TRUE), High),
      Low = if_else(is.na(Low), mean(Low, na.rm = TRUE), Low),
      Close = if_else(is.na(Close), mean(Close, na.rm = TRUE), Close),
      Adj.Close = if_else(is.na(Adj.Close), mean(Adj.Close, na.rm = TRUE), Adj.Close),
      Volume = if_else(is.na(Volume), mean(Volume, na.rm = TRUE), Volume)
    )
}

# Apply the function to your data frame
df1 <- fill_na_with_mean(df);df1
```


```{r}

# Covariance between Open and Close
cov_open_close <- cov(df1$Open, df1$Close);cov_open_close

```

```{r}
# Calculate the five-number summary for each column
summary_df <- sapply(df1[, c("Open", "High", "Low", "Close", "Adj.Close", "Volume")], fivenum);summary_df

```
```{r}
var(df1)
```
```{r}
# Quartiles
q1_volume <- quantile(df1$Volume, probs = 0.25);q1_volume
q2_volume <- quantile(df1$Volume, probs = 0.5);q2_volume
q3_volume <- quantile(df1$Volume, probs = 0.75);q3_volume
```
```{r}
# Correlation between Open and Close
cor_open_close <- cor(df1$Open, df1$Close);cor_open_close
```

```{r}
# Function to calculate z-scores
calculate_z_scores <- function(df) {
  # Exclude the Date column and calculate z-scores for the numeric columns
  df_numeric <- df %>% select(-Date)
  df_z_scores <- as.data.frame(scale(df_numeric))
  
  # Add the Date column back
  df_z_scores <- cbind(Date = df$Date, df_z_scores)
  
  return(df_z_scores)
}

# Apply the function to your data frame
df_z_scores <- calculate_z_scores(df)

# View the result
print(df_z_scores)
```

```{r}
library(e1071)
```

```{r}
# Function to classify skewness
classify_skewness <- function(skew) {
  if (skew <= 0.5 & skew >= -0.5) {
    return("lowly skewed")
  } else if (skew > 1 | skew < -1) {
    return("highly skewed")
  } else {
    return("moderately skewed")
  }
}

# Apply the skewness logic to each numeric column
skewness_results <- df %>%
  select(-Date) %>%  # Exclude the Date column
  summarise(across(everything(), ~ {
    skew = skewness(.)
    classification = classify_skewness(skew)
    return(classification)
  }))

# Print the results
print(skewness_results)
```
```{r}
# Function to classify kurtosis
classify_kurtosis <- function(kurt) {
  if (kurt < 3) {
    return("platykurtic (less peaked)")
  } else if (kurt == 3) {
    return("mesokurtic (normal distribution)")
  } else {
    return("leptokurtic (more peaked)")
  }
}
df1$Date=as.Date(df1$Date)
# Apply the kurtosis logic to each numeric column
kurtosis_results <- df1 %>%
  select(-Date) %>%  # Exclude the Date column
  summarise(across(everything(), ~ {
    kurt = kurtosis(.)
    classification = classify_kurtosis(kurt)
    return(classification)
  }))

# Print the results
print(kurtosis_results)
```

```{r}
library(ggplot2)
```
```{r}
# Histogram of Closing Prices
ggplot(df, aes(x = Close)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black") +
  labs(title = "Histogram of Closing Prices", x = "Close", y = "Frequency")
```


```{r}
library(lubridate)
```



```{r}
# Convert Date column to Date type (if not already)
df$Date <- as.Date(df$Date)

# Extract month from Date
df <- df %>%
  mutate(Month = month(Date, label = TRUE))

# Summarize volume by month
df_pie <- df %>%
  group_by(Month) %>%
  summarise(Total_Volume = sum(Volume))

# Pie Chart
ggplot(df_pie, aes(x = "", y = Total_Volume, fill = Month)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar(theta = "y") +
  labs(title = "Pie Chart of Total Volume by Month", x = "", y = "") +
  theme_void()  # Remove axes
```

```{r}
library(scales)
```


```{r}
# Convert Date column to Date type (if not already)
df$Date <- as.Date(df$Date)

# Bar Chart of Volume
ggplot(df, aes(x = Date, y = Volume)) +
  geom_bar(stat = "identity", fill = "blue") +
  labs(title = "Bar Chart of Volume", x = "Date", y = "Volume") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

```{r}
# Scatter Plot of Open vs. Close Prices
ggplot(df, aes(x = Open, y = Close)) +
  geom_point(color = "blue") +
  labs(title = "Scatter Plot of Open vs. Close Prices", x = "Open", y = "Close")

```
```{r}
# Calculate the Mahalanobis distance
mahalanobis_distance <- function(df) {
  # Select the Open and Close columns
  data <- df[, c("Open", "Close")]
  
  # Calculate the mean and covariance of the data
  mean_values <- colMeans(data)
  cov_matrix <- cov(data)
  
  # Check if the covariance matrix is near-singular
  if (det(cov_matrix) == 0) {
    stop("Covariance matrix is singular, cannot compute Mahalanobis distance")
  }
  
  # Add a small constant to the diagonal of the covariance matrix to regularize it
  small_constant <- 1e-6
  cov_matrix <- cov_matrix + diag(small_constant, ncol(cov_matrix))
  
  # Calculate the Mahalanobis distance
  mahalanobis_dist <- mahalanobis(data, mean_values, cov_matrix)
  
  return(mahalanobis_dist)
}

mahalanobis_distance(df)
  

```
```{r}

ggplot(df, aes(x = Open, y = Close)) +
  geom_line() +
  labs(x = "Timestamp", y = "Value", title = "Time Series Plot")

```
```{r}
# Convert Date column to Date type (if not already)
df$Date <- as.Date(df$Date)

# Time Series Plot of Closing Prices
ggplot(df, aes(x = Date, y = Close)) +
  geom_line(color = "blue") +
  labs(title = "Time Series Plot of Closing Prices", x = "Date", y = "Close")
```

```{r}
library(dbscan)
```
```{r}
# Select numeric columns for clustering
df_numeric <- df %>%
  select(Open, High, Low, Close, Adj.Close, Volume)

# Standardize the data (optional but recommended)
df_scaled <- scale(df_numeric)

# Apply DBSCAN
dbscan_result <- dbscan(df_scaled, eps = 0.5, minPts = 3)

# Print the DBSCAN result
print(dbscan_result)

# Add cluster assignments to the original data frame
df$Cluster <- factor(dbscan_result$cluster)

# Visualize the clusters (using the first two principal components)
pca_result <- prcomp(df_scaled)
df_pca <- data.frame(pca_result$x, Cluster = df$Cluster)

```

```{r}
ggplot(df_pca, aes(x = PC1, y = PC2, color = Cluster)) +
  geom_point(size = 3) +
  labs(title = "DBSCAN Clustering Results", x = "Principal Component 1", y = "Principal Component 2") +
  theme_minimal()
```
```{r}
df$Date=as.Date(df$Date)
# Select numeric columns for clustering
df_numeric <- df %>%
  select(Open, High, Low, Close, Adj.Close, Volume)

# Standardize the data (optional but recommended)
df_scaled <- scale(df_numeric)

# Apply k-means clustering
set.seed(123)  # Set seed for reproducibility
kmeans_result <- kmeans(df_scaled, centers = 3, nstart = 25)

# Print the k-means result
print(kmeans_result)

# Add cluster assignments to the original data frame
df$Cluster <- factor(kmeans_result$cluster)

# Visualize the clusters (using the first two principal components)
pca_result <- prcomp(df_scaled)
df_pca <- data.frame(pca_result$x, Cluster = df$Cluster)
```
```{r}
ggplot(df_pca, aes(x = PC1, y = PC2, color = Cluster)) +
  geom_point(size = 3) +
  labs(title = "K-means Clustering Results", x = "Principal Component 1", y = "Principal Component 2") +
  theme_minimal()
```
```{r}
df$Date=as.Date(df$Date)
df
```
```{r}
library(klaR)
# Convert numeric columns to categorical (example using quartiles)
df_categorical <- df %>%
  mutate(
    Open = cut(Open, breaks = quantile(Open, probs = seq(0, 1, 0.25)), include.lowest = TRUE),
    High = cut(High, breaks = quantile(High, probs = seq(0, 1, 0.25)), include.lowest = TRUE),
    Low = cut(Low, breaks = quantile(Low, probs = seq(0, 1, 0.25)), include.lowest = TRUE),
    Close = cut(Close, breaks = quantile(Close, probs = seq(0, 1, 0.25)), include.lowest = TRUE),
    Adj.Close = cut(Adj.Close, breaks = quantile(Adj.Close, probs = seq(0, 1, 0.25)), include.lowest = TRUE),
    Volume = cut(Volume, breaks = quantile(Volume, probs = seq(0, 1, 0.25)), include.lowest = TRUE)
  )

# Apply k-modes clustering
set.seed(123)  # Set seed for reproducibility
kmodes_result <- kmodes(df_categorical[, -1], modes = 3, iter.max = 10, weighted = FALSE)

# Print the k-modes result
print(kmodes_result)

# Add cluster assignments to the original data frame
df$Cluster <- factor(kmodes_result$cluster)
```


```{r}
df$Date=as.Date(df$Date)
# Categorize volumes (for example purposes, we'll create arbitrary categories)
df <- df %>%
  mutate(Volume_Category = cut(Volume, 
                               breaks = c(0, 1100, 1300, 1500, 1700, Inf), 
                               labels = c("Very Low", "Low", "Medium", "High", "Very High")))

# Summarize counts by volume category
df_pareto <- df %>%
  group_by(Volume_Category) %>%
  summarise(Count = n()) %>%
  arrange(desc(Count)) %>%
  mutate(Cumulative = cumsum(Count),
         Cumulative_Percentage = Cumulative / sum(Count) * 100)
print(df_pareto)

```
```{r}
# Pareto Chart
ggplot(df_pareto, aes(x = reorder(Volume_Category, -Count), y = Count)) +
  geom_bar(stat = "identity", fill = "white") +
  geom_line(aes(y = Cumulative_Percentage, group = 1, color = "red")) +
  geom_point(aes(y = Cumulative_Percentage, color = "red")) +
  scale_y_continuous(sec.axis = sec_axis(~ ., name = "Cumulative Percentage", labels = percent_format())) +
  labs(title = "Pareto Chart of Volume Categories", x = "Volume Category", y = "Count") +
  theme(legend.position = "none")

```

```{r}
library(klaR)
```

```{r}
# Convert numeric columns to categorical (example using quartiles)
df_categorical <- df %>%
  mutate(
    Open = cut(Open, breaks = quantile(Open, probs = seq(0, 1, 0.25)), include.lowest = TRUE),
    High = cut(High, breaks = quantile(High, probs = seq(0, 1, 0.25)), include.lowest = TRUE),
    Low = cut(Low, breaks = quantile(Low, probs = seq(0, 1, 0.25)), include.lowest = TRUE),
    Close = cut(Close, breaks = quantile(Close, probs = seq(0, 1, 0.25)), include.lowest = TRUE),
    Adj.Close = cut(Adj.Close, breaks = quantile(Adj.Close, probs = seq(0, 1, 0.25)), include.lowest = TRUE),
    Volume = cut(Volume, breaks = quantile(Volume, probs = seq(0, 1, 0.25)), include.lowest = TRUE)
  )

# Apply k-modes clustering
set.seed(123)  # Set seed for reproducibility
kmodes_result <- kmodes(df_categorical[, -1], modes = 3, iter.max = 10, weighted = FALSE)

# Print the k-modes result
print(kmodes_result)

# Add cluster assignments to the original data frame
df$Cluster <- factor(kmodes_result$cluster)
```

```{r}
library(ggcorrplot)
```
```{r}
# Exclude the Date column
numeric_columns <- df[, 2:7]

# Calculate the Pearson correlation matrix
correlation_matrix <- cor(numeric_columns, method = "pearson")

# Print the correlation matrix
print(correlation_matrix)
```
```{r}
# Visualize the correlation matrix
ggcorrplot(correlation_matrix, lab = TRUE)
```
```{r}
library(reshape2)
```
```{r}
# Bin the Close and Volume columns into categorical variables
df$Close_bin <- cut(df$Close, breaks = 3, labels = c("Low", "Medium", "High"))
df$Volume_bin <- cut(df$Volume, breaks = 3, labels = c("Low", "Medium", "High"))

# Create a contingency table
contingency_table <- table(df$Close_bin, df$Volume_bin)

# Perform the Chi-square test
chi_square_test <- chisq.test(contingency_table)

# Print the test results
print(chi_square_test)
```
```{r}
# Melt the contingency table for visualization
contingency_table_melted <- melt(contingency_table)

# Plot the contingency table using ggplot2
ggplot(contingency_table_melted, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  geom_text(aes(label = value), vjust = 1) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(x = "Close Bin", y = "Volume Bin", fill = "Count") +
  ggtitle("Contingency Table of Close Price and Volume Bins") +
  theme_minimal()

```
```{r}
library(caret)
```
```{r}
ggplot(df, aes(x=NULL)) +
  geom_boxplot(aes(y=Open, fill="Open"), alpha=0.5) +
  geom_boxplot(aes(y=High, fill="High"), alpha=0.5) +
  geom_boxplot(aes(y=Low, fill="Low"), alpha=0.5) +
  geom_boxplot(aes(y=Close, fill="Close"), alpha=0.5) +
  scale_fill_discrete(name="Legend") +
  labs(title="Box Plot of Stock Prices",
       x="values",
       y="Prices") +
  theme_minimal()
```
```{r}
library(readr)
```


```{r}
# Load necessary libraries
library(MASS)  # For the stepAIC function

# Load your data (replace 'your_data.csv' with your actual file)
data <- read.csv(file)

# Check for missing values
sum(is.na(data))

# If necessary, handle missing values (e.g., by removing rows with NA values)
data <- na.omit(data)

# Define the response variable and predictor variables
# Assume we are predicting the 'Close' price based on other variables
response <- "Close"
predictors <- c("Open", "High", "Low", "Adj.Close", "Volume")

# Create an empty model (just the intercept)
initial_model <- lm(as.formula(paste(response, "~ 1")), data = data)

# Create the full model (using all predictors)
full_model <- lm(as.formula(paste(response, "~", paste(predictors, collapse = " + "))), data = data)

# Perform forward selection using the step function
forward_selection <- step(initial_model, direction = "forward", scope = list(lower = initial_model, upper = full_model))

# Display the summary of the final model
summary(forward_selection)

```

```{r}
# Load necessary libraries
library(MASS)  # For the stepAIC function

# Load your data (replace 'your_data.csv' with your actual file)
data <- read.csv(file)

# Check for missing values
sum(is.na(data))

# If necessary, handle missing values (e.g., by removing rows with NA values)
data <- na.omit(data)

# Define the response variable and predictor variables
# Assume we are predicting the 'Close' price based on other variables
response <- "Close"
predictors <- c("Open", "High", "Low", "Adj.Close", "Volume")

# Create the full model (using all predictors)
full_model <- lm(as.formula(paste(response, "~", paste(predictors, collapse = " + "))), data = data)

# Perform backward elimination using the step function
backward_elimination <- step(full_model, direction = "backward")

# Display the summary of the final model
summary(backward_elimination)

```

